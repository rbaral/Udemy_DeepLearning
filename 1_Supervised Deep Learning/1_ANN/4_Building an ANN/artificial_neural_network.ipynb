{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Building basic Neural Network</b>\n",
    "\n",
    "In this notebook, we build a basic ANN and analyze the Churn_Modeling data to demonstrate a classification task.\n",
    "\n",
    "We will use Keras library to build the NN, so make sure that this is installed. As the Keras is like an abstract wrapper for TensorFlow or Theano, the Keras requires these based on which Framework - TensorFlow or Theano you will be asking Keras to use for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load the data using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "#lets see a chunk of the data\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the available fields, the field like customerid, surname has no role on the target variable. So, we ignore them in our processing. We consider all the rows but the fields are selected from the field 3 (credit score) to field 12 (Estimated salary).\n",
    "\n",
    "Our target variable is the field 13  (Exited).\n",
    "We initialie the X and y variables to hold the independent and dependent/target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have the gender and country as categorical fields. As computing is easier in numerical form, we encode them to numerical form using the sklearn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[619 'France' 'Female' 42 2 0.0 1 1 1 101348.88]\n",
      "[608 'Spain' 'Female' 41 1 83807.86 1 0 1 112542.58]\n",
      "[502 'France' 'Female' 42 8 159660.8 3 1 0 113931.57]\n",
      "[699 'France' 'Female' 39 1 0.0 2 0 0 93826.63]\n",
      "[850 'Spain' 'Female' 43 2 125510.82 1 1 1 79084.1]\n",
      "[645 'Spain' 'Male' 44 8 113755.78 2 1 0 149756.71]\n",
      "[822 'France' 'Male' 50 7 0.0 2 1 1 10062.8]\n",
      "[376 'Germany' 'Female' 29 4 115046.74 4 1 0 119346.88]\n",
      "[501 'France' 'Male' 44 4 142051.07 2 0 1 74940.5]\n",
      "[684 'France' 'Male' 27 2 134603.88 1 1 1 71725.73]\n"
     ]
    }
   ],
   "source": [
    "#lets see the number of independent variables\n",
    "print(len(X[0]))\n",
    "#lets see some of the entries\n",
    "for i in range(10):\n",
    "    print(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2} {0, 1}\n"
     ]
    }
   ],
   "source": [
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "#create the encoder for the first column to be encoded\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "#encode the first column using this encoder\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "#create an encoder for second column and encode it\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "#lets see the number of categories in the field1 and field2\n",
    "print(set(X[:,1]),set(X[:,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have three categories for the field 1 (country) and two categories for the field 2 (gender).\n",
    "\n",
    "As the country column has more than two categories, it should be noted that they are not comparable, i.e. 0 is not less than 1 and 1 is not greater than 0, and so on. In order to avoid the confusion, we use a different technique called One-Hot Encoding, which encodes the country field (field 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "[  1.00000000e+00   0.00000000e+00   0.00000000e+00   6.19000000e+02\n",
      "   0.00000000e+00   4.20000000e+01   2.00000000e+00   0.00000000e+00\n",
      "   1.00000000e+00   1.00000000e+00   1.00000000e+00   1.01348880e+05]\n",
      "[  0.00000000e+00   0.00000000e+00   1.00000000e+00   6.08000000e+02\n",
      "   0.00000000e+00   4.10000000e+01   1.00000000e+00   8.38078600e+04\n",
      "   1.00000000e+00   0.00000000e+00   1.00000000e+00   1.12542580e+05]\n",
      "[  1.00000000e+00   0.00000000e+00   0.00000000e+00   5.02000000e+02\n",
      "   0.00000000e+00   4.20000000e+01   8.00000000e+00   1.59660800e+05\n",
      "   3.00000000e+00   1.00000000e+00   0.00000000e+00   1.13931570e+05]\n",
      "[  1.00000000e+00   0.00000000e+00   0.00000000e+00   6.99000000e+02\n",
      "   0.00000000e+00   3.90000000e+01   1.00000000e+00   0.00000000e+00\n",
      "   2.00000000e+00   0.00000000e+00   0.00000000e+00   9.38266300e+04]\n",
      "[  0.00000000e+00   0.00000000e+00   1.00000000e+00   8.50000000e+02\n",
      "   0.00000000e+00   4.30000000e+01   2.00000000e+00   1.25510820e+05\n",
      "   1.00000000e+00   1.00000000e+00   1.00000000e+00   7.90841000e+04]\n",
      "[  0.00000000e+00   0.00000000e+00   1.00000000e+00   6.45000000e+02\n",
      "   1.00000000e+00   4.40000000e+01   8.00000000e+00   1.13755780e+05\n",
      "   2.00000000e+00   1.00000000e+00   0.00000000e+00   1.49756710e+05]\n",
      "[  1.00000000e+00   0.00000000e+00   0.00000000e+00   8.22000000e+02\n",
      "   1.00000000e+00   5.00000000e+01   7.00000000e+00   0.00000000e+00\n",
      "   2.00000000e+00   1.00000000e+00   1.00000000e+00   1.00628000e+04]\n",
      "[  0.00000000e+00   1.00000000e+00   0.00000000e+00   3.76000000e+02\n",
      "   0.00000000e+00   2.90000000e+01   4.00000000e+00   1.15046740e+05\n",
      "   4.00000000e+00   1.00000000e+00   0.00000000e+00   1.19346880e+05]\n",
      "[  1.00000000e+00   0.00000000e+00   0.00000000e+00   5.01000000e+02\n",
      "   1.00000000e+00   4.40000000e+01   4.00000000e+00   1.42051070e+05\n",
      "   2.00000000e+00   0.00000000e+00   1.00000000e+00   7.49405000e+04]\n",
      "[  1.00000000e+00   0.00000000e+00   0.00000000e+00   6.84000000e+02\n",
      "   1.00000000e+00   2.70000000e+01   2.00000000e+00   1.34603880e+05\n",
      "   1.00000000e+00   1.00000000e+00   1.00000000e+00   7.17257300e+04]\n"
     ]
    }
   ],
   "source": [
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "#lets see the number of independent variables\n",
    "print(len(X[0]))\n",
    "#lets see some of the entries\n",
    "for i in range(10):\n",
    "    print(X[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Dummy variable Trap </b>\n",
    "\n",
    "\n",
    "It looks like we got 2 more fields after doing the One-Hot encoding of the countryh field. This is because the one field for country is now represented by one-hot encoding vector (we need three element vector to represent three different values).\n",
    "\n",
    "After LabelEncoding and Hot-encoding, we got many dummy variables in our data. By including dummy variable in a regression model however, one should be careful of the Dummy Variable Trap. The Dummy Variable trap is a scenario in which the independent variables are multicollinear - a scenario in which two or more variables are highly correlated; in simple terms one variable can be predicted from the others.\n",
    "\n",
    "So we remove one of the dummy variable to avoid the situation of falling into the dummy variable trap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Feature Scaling </b>\n",
    "\n",
    "In order to prevent one variable dominating the other variable, we need to perform the feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Building Neural Network</b>\n",
    "\n",
    "Now we are going to create our first basic NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Adding a Layer to the Network</b>\n",
    "\n",
    "Now we are adding a layer in our network. The number of nodes in the input layer is denoted as input_dim (which is 11 in our case because we have 11 input variables). The output_dim is the number of output nodes of this layer. As this is the first layer we are adding, the output_dim will be the input for the next layer. This means in our hidden layer there will be 6 input nodes.\n",
    "The kernel_initializer is set to uniform to make sure that the weights for this layer are uniformly initialized. The activation function for this layer is set to Rectified Linear Units (RELU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rbaral\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", kernel_initializer=\"uniform\", units=6, input_dim=11)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(input_dim = 11, output_dim = 6, kernel_initializer = 'uniform', activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rbaral\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.1)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#adding a dropout layer\n",
    "# p is the fraction of neurons to be dropped out, here we start with 10%, we can incerase it by 10% if the overfitting is not\n",
    "# resolved. We need to ensure that p is not too high (>0.5) else it will introduce the situation of underfitting.\n",
    "classifier.add(Dropout(p= 0.1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Adding another hidden layer</b>\n",
    "\n",
    "Now lets add another layer to our network. The parameter \"units\" resembles the number of nodes in this layer. As we already added a layer before this layer, the inputs of this layer is automatically taken by the Keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rbaral\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.1)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "#add a dropout layer\n",
    "classifier.add(Dropout(p= 0.1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Adding an output layer</b>\n",
    "\n",
    "Now we are going to add an output layer to our network. Our output layer just outputs a single value (either 1 or 0) so we have units=1 that resembles the output of this layer. The weight of this layer is also set to uniform. Inorder to convert the probability of the binary classifier, we use the sigmoid function as the activation function of this layer. \n",
    "\n",
    "As a note, if our classifier needs to classify the data into three classes, then we need to set units=2 and set the activation function to softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Compiling the network</b>\n",
    "\n",
    "We need to compile the network before it can be executed. We can provide additional parameters during we compile the network.\n",
    "\n",
    "The optimizer is to select the algorithm that defines the optimal set of weights in the network. The \"adam\" is one of the very popular stochastic algorithm for the weight initialization.\n",
    "\n",
    "\n",
    "The another parameter is the loss function. We select the binary cross entropy as our loss function because we have two possible outputs. This loss function is needed because we need to select a logarithmic loss function for the sigmoid function which uses the stochastic gradient descent. Using the sigmoid function with stochastic gradient descent is just like a logistic regression model whose loss is not the sum of square of the error but is determined by the logarithmic loss.\n",
    "\n",
    "As a note, if we had more than two outputs then we could have selected categorical_crossentropy as our loss function.\n",
    "\n",
    "\n",
    "The metrics to measure the performance of the model is specified by the \"metrics\" parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Fit the network to the data</b>\n",
    "\n",
    "We define the batch size of 10 and epochs of 100 to this network.\n",
    "\n",
    "The batch size indiciates how many observations is to be used before we update the weigth parameter.\n",
    "\n",
    "The parameter epoch indicates how many round of iterations we need to run with the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4944 - acc: 0.7960     \n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4354 - acc: 0.7960     \n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4345 - acc: 0.7960     \n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4287 - acc: 0.7960     - ETA: 1s - loss\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4299 - acc: 0.7960     - ETA: 0s - loss: 0.4471\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4292 - acc: 0.7960     \n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4268 - acc: 0.8067     \n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4270 - acc: 0.8212     \n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4286 - acc: 0.8242     \n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4288 - acc: 0.8259     \n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4294 - acc: 0.8255     \n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4251 - acc: 0.8281     \n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4260 - acc: 0.8267     \n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4279 - acc: 0.8239     \n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4249 - acc: 0.8264     - ETA: 0s - loss: \n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4275 - acc: 0.8245     - ETA: 0s - loss: 0.\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4271 - acc: 0.8260     \n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4281 - acc: 0.8271     \n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4252 - acc: 0.8260     \n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4271 - acc: 0.8266     \n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4260 - acc: 0.8275     \n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4246 - acc: 0.8286     \n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4242 - acc: 0.8257     \n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4263 - acc: 0.8289     \n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4239 - acc: 0.8301     \n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4232 - acc: 0.8286     \n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4289 - acc: 0.8257     \n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4263 - acc: 0.8296     \n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4249 - acc: 0.8285     \n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4248 - acc: 0.8302     - ETA: 0s - loss: 0.4272 - acc: 0\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4246 - acc: 0.8272     \n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4252 - acc: 0.8287     \n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4259 - acc: 0.8280     - ETA: 0s - loss: 0.4132  - ETA: 0s - loss: 0.4255 - acc: 0\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4251 - acc: 0.8269     \n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4253 - acc: 0.8291     \n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4235 - acc: 0.8305     - ETA: 0s - loss: 0.4248 - acc: 0.\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4274 - acc: 0.8282     \n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4243 - acc: 0.8280     \n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4211 - acc: 0.8300     - ETA: 0s - loss: 0.4128 -\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4262 - acc: 0.8282     - ETA: 1s - l\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4236 - acc: 0.8297     \n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4246 - acc: 0.8290     \n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4247 - acc: 0.8277     \n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4236 - acc: 0.8279     \n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4260 - acc: 0.8311     \n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4244 - acc: 0.8275     \n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4272 - acc: 0.8285     \n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4249 - acc: 0.8287     \n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4259 - acc: 0.8294     \n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4253 - acc: 0.8286     \n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4229 - acc: 0.8291     \n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4267 - acc: 0.8292     \n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4259 - acc: 0.8282     \n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4234 - acc: 0.8285     \n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4239 - acc: 0.8284     \n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4250 - acc: 0.8321     \n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4216 - acc: 0.8302     \n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4236 - acc: 0.8296     \n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4246 - acc: 0.8291     \n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4229 - acc: 0.8296     - ETA: 0s - loss: 0.4228 - acc: 0.831 - ETA: 0s - loss: 0.4230 -\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4233 - acc: 0.8285     \n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4247 - acc: 0.8300     - ETA: 0s - loss: 0.4252 - acc: 0.829\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4221 - acc: 0.8314     \n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4237 - acc: 0.8302     \n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4211 - acc: 0.8316     \n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4210 - acc: 0.8319     - ETA: 0s - loss: 0.4198 - acc: 0.8 - ETA: 0s - loss: 0.4166 - acc\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4215 - acc: 0.8306     - ETA: 0s - loss: 0.4197 \n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4267 - acc: 0.8297     - ETA: 0s - loss: 0.43\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4243 - acc: 0.8309     \n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4245 - acc: 0.8287     - ETA: 0s - loss: 0.\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4241 - acc: 0.8306     \n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4231 - acc: 0.8314     \n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4229 - acc: 0.8316     \n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4209 - acc: 0.8302     \n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4222 - acc: 0.8305     \n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4212 - acc: 0.8319     \n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4214 - acc: 0.8296     \n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4231 - acc: 0.8311     \n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4220 - acc: 0.8314     \n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4244 - acc: 0.8331     \n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4271 - acc: 0.8305     \n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s - loss: 0.4212 - acc: 0.8330     \n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4240 - acc: 0.8279     \n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4234 - acc: 0.8300     \n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4254 - acc: 0.8299     \n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4236 - acc: 0.8309     \n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4231 - acc: 0.8302     - ETA: 0s - loss: 0.4225 - acc: 0.8\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4246 - acc: 0.8306     \n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4253 - acc: 0.8304     \n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4200 - acc: 0.8327     \n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4235 - acc: 0.8280     \n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4214 - acc: 0.8306     \n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4243 - acc: 0.8314     \n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4232 - acc: 0.8319     - ETA: 0s - loss: 0.4166 - acc\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4224 - acc: 0.8312     \n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4230 - acc: 0.8285     \n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4248 - acc: 0.8311     \n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4241 - acc: 0.8297     \n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4210 - acc: 0.8311     \n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4250 - acc: 0.8286     - ETA: 0s - loss: 0.4228 - acc: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xa190828>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b> Predictions </b>\n",
    "\n",
    "Now we are going to predict if a customer is going to leave a bank or not. For this, we use a very simple method predict() that is available in the classifier object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "#we can  convert the predictions to True or False values, by comparing the predicted value\n",
    "# to a standard value of 0.5. If higher then predict is True else it is False.\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]\n",
      " [False]\n",
      " [False]\n",
      " ..., \n",
      " [False]\n",
      " [False]\n",
      " [False]]\n"
     ]
    }
   ],
   "source": [
    "#lets see the vlaue of predictions\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Making the confusion matrix</b>\n",
    "\n",
    "We can use the sklearn package to create the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is often useful to measure the peformance of the model. We can use the confusion matrix to find the True-positives, False-positives, True-Negatives and False-Negatives. These values can be used to compute other metrics like accuracy, precision, recall, F-Score, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1556   39]\n",
      " [ 277  128]]\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.842\n"
     ]
    }
   ],
   "source": [
    "accuracy = (cm[0][0] + cm[1][1])/(np.sum(cm[0]) + np.sum(cm[1]))\n",
    "print(\"accuracy is:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compute the other metrics:\n",
    "\n",
    "precision = tp/(tp+fp),\n",
    "recall = tp/(tp+fn), \n",
    "and \n",
    "f-score = 2*precision*recall/(precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision is: 0.848881614839\n",
      "recall is: 0.975548589342\n",
      "fscore is: 0.907817969662\n"
     ]
    }
   ],
   "source": [
    "prec = cm[0][0]/(cm[0][0] + cm[1][0])\n",
    "rec = cm[0][0]/(cm[0][0] + cm[0][1])\n",
    "fscore = 2*prec*rec/(prec + rec)\n",
    "\n",
    "print(\"precision is:\",prec)\n",
    "print(\"recall is:\",rec)\n",
    "print(\"fscore is:\",fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Homework </b>\n",
    "\n",
    "Use our ANN model to predict if the customer with the following informations will leave the bank: \n",
    "\n",
    "Geography: France\n",
    "\n",
    "Credit Score: 600\n",
    "\n",
    "Gender: Male\n",
    "\n",
    "Age: 40 years old\n",
    "\n",
    "Tenure: 3 years\n",
    "\n",
    "Balance: $60000\n",
    "\n",
    "Number of Products: 2\n",
    "\n",
    "Does this customer have a credit card ? Yes\n",
    "\n",
    "Is this customer an Active Member: Yes\n",
    "\n",
    "Estimated Salary: $50000\n",
    "\n",
    "So should we say goodbye to that customer ?\n",
    "\n",
    "<b> Transforming the data to the required format </b>\n",
    "\n",
    "Before the given data can be used in our network, we need to transform this data in the format that is understood by the network. To recap, we did label encoding for categorical variables,so we need to transform our data in the similar format.\n",
    "Lets see how we transform this data and feed it to our network to make the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rbaral\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#from the label encoding, geography France is taken as 0, 0. The gender Male is taken as 1. \n",
    "#Having a credit card is represented by 1\n",
    "# being an active member is denoted by 1. The rest of the values are fed without the units.\n",
    "# The standard scaling and transformation is done on the array which includes all these values in the correct order. We can see\n",
    "# one of the earlier entries to ensure we maintain the proper order.\n",
    "\n",
    "#we use the row vector by having the values in double braces [[]]\n",
    "new_prediction = classifier.predict(sc.transform(np.array([[0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])))\n",
    "new_prediction = (new_prediction > 0.5)\n",
    "print(new_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it looks like the given client is more likely stay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> K-fold cross validation</b>\n",
    "\n",
    "Lets use cross validation to make sure that we are getting consistent and reliable result. We use KerasClassifier which is a wrapper for the scikit-learn and is useful in measuring the performance using the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lets import the required packages\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#lets create a method that creates the basic classifier as we had before\n",
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Adding the input layer and the first hidden layer\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "\n",
    "    # Adding the second hidden layer\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "    # Adding the output layer\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "    # Compiling the ANN\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "#the keras classifier takes a method that provides the basic classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, epochs = 100)\n",
    "#cv denots the number of folds to be used in cross validation, n_jobs= -1 means use all the available cpus for parallel exeuction\n",
    "accuracies = cross_val_score(estimator = classifier, X =X_train, y= y_train, cv =10, n_jobs = -1)\n",
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()\n",
    "print(mean)\n",
    "print(variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Dropout regularization</b>\n",
    "\n",
    "This is a useful technique that helps to improve the performance by minimizing the overfitting. Overfitting occurs when we get high variance in the accuracies in the cross validation. It also occurs when the model performs good in the training data but not in the test data.\n",
    "\n",
    "The term dropout means at every iteration some nodes are randomly disabled to prevent them from being too much dependent and having dependent correlation. Using this technique, the network learns several independent correlation because every time we have differnt configurations of the neurons.\n",
    "\n",
    "Keras provides an easier method to add the dropout regularization. After adding a layer, we can just add a line classifier.add(Dropout(p= 0.1)) to add the dropout of fraction of p neurons in the layer.\n",
    "\n",
    "\n",
    "<b> Parameter tuning </b>\n",
    "\n",
    "We can tune different hyperparameters (e.g., batch size, epoch, optimizer, and so on). The hyperparameters are the one that were kept fixed in our model (e.g. batch size, epoch, etc.). The other parameters like the weight of neurons in each layer were dynamically selected so they are just called parameters.\n",
    "\n",
    "Here we are going to play around with a set of hyperparameters and see which one of them give the best solution.\n",
    "\n",
    "Scikit-learn provides a GridSearchCV class which has the required methods for the parameter tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we build a method that prepares the classifier for us.\n",
    "#this method is just a copy of build_classifier from above but it takes\n",
    "# parameter to make the method more suitable for parameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def build_classifier_GridSearch(optimizer):\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Adding the input layer and the first hidden layer\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "\n",
    "    # Adding the second hidden layer\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "    # Adding the output layer\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "    # Compiling the ANN\n",
    "    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "#the keras classifier takes a method that provides the basic classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier_GridSearch)\n",
    "parameters = {'batch_size':[25, 30, 35, 40, 45],\n",
    "             'nb_epoch':[100, 200, 300],\n",
    "             'optimizer':['adam','rmsprop']}\n",
    "grid_search = GridSearchCV(estimator = classifier, \n",
    "                           param_grid = parameters,\n",
    "                          scoring = 'accuracy',\n",
    "                          cv = 10)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "\n",
    "print(\"best params are:\",best_parameters)\n",
    "print(\"best accuracy is:\",best_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
